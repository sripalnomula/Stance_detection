# -*- coding: utf-8 -*-
"""FakeArticle_detection

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dfZavxj1rRMaNzgj1GIHc4WmaY3ug67d
"""

from google.colab import drive
drive.mount('/content/drive')
import pandas as pd
import numpy as np
true = pd.read_csv('/content/drive/MyDrive/FNC_Project/true.csv')
fake = pd.read_csv('/content/drive/MyDrive/FNC_Project/fake.csv')
true.head()

fake.head()

#above one's are of true dataset so preprocessing for identification of True or fake we will add this class to the dataset.
true['class'] = 1
fake['class'] = 0
true.head()

fake.head()

true.info()

fake.info()



# Merging 'true' and 'fake' datasets
merged_df = pd.concat([true, fake], ignore_index=True)
# merged dataset
merged_df.head()

df= merged_df.copy()
df.head()

df.isnull().sum()

df.info()

df.shape

#Handling missing and duplicate values
#there are couple of hundreds of duplicates so we remove them inorder for better performance(it will also affect the probability the determining whether the title Trues or Fake with the body.)
df = df.drop_duplicates()
df.shape

# let us Check the summary statistics(5- point summary) of the dataset
df.describe()

# Checking the distribution of the 'class' column i.e., true or fake
df['class'].value_counts()



import matplotlib.pyplot as plt
# Plotting the distribution of the True or Fake articles in the  given dataset. 
plt.figure(figsize=(8, 6))
df['class'].value_counts().plot(kind='bar')
plt.title("Distribution of 'class' column")
plt.xlabel("Class")
plt.ylabel("Count")
plt.show()





import seaborn as sns
fig, ax = plt.subplots(1,2, figsize=(19, 5))
ax = sns.countplot(x = df['class'],ax=ax[0])
plt.title('Distribution of True or Fake articles')
ax.set(xticklabels=['True', 'Fake'])
plt.xlabel('News Type')
plt.ylabel('Number of News')
plt.pie(df["class"].value_counts().values,explode=[0,0],labels=df["class"].value_counts().index, autopct='%1.1f%%')
fig.show()

plt.figure(figsize = (12,8))
sns.set(style = "whitegrid",font_scale = 0.8)
chart = sns.countplot(x = "subject", data = df,color="blue")
chart.set_xticklabels(chart.get_xticklabels(), rotation=90)
plt.show()

#here is the comparition of articles with word count vs class(True or Fake)
df_ag = df[df['class'] == 1]
df_dis = df[df['class'] == 0]

word_count_ag = df_ag['text'].apply(lambda x: len(x.split()))
word_count_dis = df_dis['text'].apply(lambda x: len(x.split()))

print("Average word count for class True: ", word_count_ag.mean())
print("Average word count for class Fake: ", word_count_dis.mean())

plt.boxplot([word_count_ag, word_count_dis], labels=['True', 'Fake'])
plt.title('Comparison of Word Counts in Text for True and Fake Classes')
plt.ylabel('Word Count in Text(body of article')
plt.show()

title_word_count_ag = df_ag['title'].apply(lambda x: len(x.split()))
title_word_count_dis = df_dis['title'].apply(lambda x: len(x.split()))
plt.boxplot([word_count_ag, word_count_dis], labels=['True', 'Fake'])
plt.title('Comparison of Word Counts in Title for True and Fake Classes')
plt.ylabel('Word Count in Title of article')
plt.show()

df['subject'].value_counts()

import matplotlib.pyplot as plt
data = pd.Series(['politicsNews'] * 11272 + ['worldnews'] * 10145 + ['News']*9050 + ['politics'] * 6841 + ['left-news'] * 4459 + ['Government News']*1570 +['US_News']*783 + ['Middle-east']*778)
counts = data.value_counts()
plt.pie(counts, labels=counts.index, autopct='%1.1f%%')
plt.title('Distribution of Subject Categories')
plt.show()



import datetime as dt
#these are some outliers where it's considering NAT values we manually overide them
df['date'] = df['date'].replace(['13-Feb-18'],'February 13, 2018')
df['date'] = df['date'].replace(['14-Feb-18'],'February 14, 2018')
df['date'] = df['date'].replace(['15-Feb-18'],'February 15, 2018')
df['date'] = df['date'].replace(['16-Feb-18'],'February 16, 2018')
df['date'] = df['date'].replace(['17-Feb-18'],'February 17, 2018')
df['date'] = df['date'].replace(['18-Feb-18'],'February 18, 2018')
df['date'] = df['date'].replace(['19-Feb-18'],'February 19, 2018')

# Creating a dictionary to map abbreviated month names to full month names
month_mapping = {
    'Dec ': 'December ',
    'Nov ': 'November ',
    'Oct ': 'October ',
    'Sep ': 'September ',
    'Aug ': 'August ',
    'Jul ': 'July ',
    'Jun ': 'June ',
    'Apr ': 'April ',
    'Mar ': 'March ',
    'Feb ': 'February ',
    'Jan ': 'January '
}
# df["date"].apply(lambda x: x.replace())
for k in month_mapping.keys():
  df['date'] = df['date'].str.replace(k,month_mapping[k])

df['date'] = df['date'].str.replace(' ', '')

for i, val in enumerate(df['date']):
    df['date'].iloc[i] = pd.to_datetime(df['date'].iloc[i], format='%B%d,%Y', errors='coerce')
df.head()
df['date'] = df['date'].astype('datetime64[ns]')
import datetime as dt
df['year'] = pd.to_datetime(df['date']).dt.to_period('Y')
df['month'] = pd.to_datetime(df['date']).dt.to_period('M')

df['month'] = df['month'].astype(str)

import pandas as pd
import matplotlib.pyplot as plt

import datetime as dt

# Convert 'date' column to datetime with appropriate format handling
# df['date'] = pd.to_datetime(df['date'], format='%B %d, %Y', errors='coerce')

# # Extract year and month from 'date'
# df['year'] = df['date'].dt.to_period('Y')
# df['month'] = df['date'].dt.to_period('M')
print(df.head())
df['fake'] = df['class'].apply(lambda x: 1 if x == 0 else 0)
df['true'] = df['class'].apply(lambda x: 1 if x == 1 else 0)


# Group by 'month' and sum 'class' column
sub = df[['month', 'fake']].dropna().groupby(['month'])['fake'].sum()

# Check if 'NaT' value is present in the index before dropping
if pd.NaT in sub.index:
    sub = sub.drop(pd.NaT)

# Plot the data
plt.suptitle('Distribution of "Fake" articles')
plt.xticks(rotation=90)
plt.ylabel('Number of articles')
plt.xlabel('Month-Year')
plt.plot(sub.index.astype(str), sub.values, linewidth=2, color='blue')
plt.show()

sub = df[['month', 'true']].dropna().groupby(['month'])['true'].sum()
# Plot the data
plt.suptitle('Distribution of "True" articles')
plt.xticks(rotation=90)
plt.ylabel('Number of articles')
plt.xlabel('Month-Year')
plt.plot(sub.index.astype(str), sub.values, linewidth=2, color='blue')
plt.show()

df['date'] = pd.to_datetime(df['date'], errors='coerce')
plt.hist(df[df['class']==1]['date'], bins=10, color='blue')
plt.xlabel('Date')
plt.ylabel('No. of Articles')
plt.title('Histogram of Class = True with Respect to Date')
plt.xticks(rotation=45)
plt.show()

df['date'] = pd.to_datetime(df['date'], errors='coerce')
plt.hist(df[df['class']==0]['date'], bins=10, color='blue')
plt.xlabel('Date')
plt.ylabel('No. of Articles')
plt.title('Histogram of Class = Fake with Respect to Date')
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(8, 6))
plt.scatter(df['title'].str.len(), df['text'].str.len(),color="green")
plt.title("Relationship between title and text length")
plt.xlabel("Title length (in characters)")
plt.ylabel("Text length (in characters)")
plt.show()

from collections import Counter

def plot_top_words(df, label):
    text = ' '.join(df[df['class']==label]['text'].astype(str).tolist())
    word_freq = Counter(text.split()).most_common(10)
    words = [pair[0] for pair in word_freq]
    counts = [pair[1] for pair in word_freq]
    plt.figure(figsize=(8, 6))
    plt.bar(words, counts)
    plt.title(f"Top 10 most frequent words in {label} news")
    plt.xlabel("Word")
    plt.ylabel("Count")
    plt.show()

plot_top_words(df, 0)
plot_top_words(df, 1)

df['text_len'] = df['text'].str.len()
df['num_exclamations'] = df['text'].str.count('!')
df['title_len'] = df['title'].str.len()
#print(df.head())
corr_matrix = df[['title_len', 'text_len', 'num_exclamations']].corr()
plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

import nltk
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
from nltk import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
# Downloading necessary NLTK data
nltk.download('punkt')
nltk.download('words')
nltk.download('stopwords')
# Apply word tokenization
nlp = df
nlp['title'] = nlp['title'].apply(lambda x: word_tokenize(str(x)))
# here we apply Snowball stemming
snowball = SnowballStemmer(language='english')
nlp['title'] = nlp['title'].apply(lambda x: [snowball.stem(y) for y in x])
nlp['title'] = nlp['title'].apply(lambda x: ' '.join(x))
# here we remove stopwords in the below step
stopwords = stopwords.words('english')
nlp['title'] = nlp['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords]))
# Applying TF-IDF vectorization
tfidf = TfidfVectorizer()
X_text = tfidf.fit_transform(nlp['title'])

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X_text, nlp['class'], test_size=0.33, random_state=1)

def remove_b_prefix(string):
    return string.lstrip('b')

true['modified_text'] = true['text'].apply(remove_b_prefix)
fake['modified_text'] = fake['text'].apply(remove_b_prefix)

import re
def clean_tweet_text(tweet_text):
    pattern = r"(@[\w]+)|(#)|(https?://[A-Za-z0-9./]*)|([0-9]+|\n)|([^0-9A-Za-z!? \t])|(\w+:\/\/\S+)|('rt')"
    clean_text = re.sub(pattern, '', tweet_text)
    return clean_text.lower()

true['modified_text'] = true['modified_text'].apply(clean_tweet_text)
print(true['modified_text'])
fake['modified_text'] = fake['modified_text'].apply(clean_tweet_text)
print(fake['modified_text'])

import string
from nltk.corpus import stopwords
exclude = set(string.punctuation)
my_stopwords=['i','im','is','am','are','was','he','she','me','you','your',
             'by','for','of','they','them','this','that','these','those',
             'what','how','where','why''the','a','an','and','has','had','have','be','in']

def preprocess_tweet_text(tweet_text):
    words = tweet_text.split()
    words = [word for word in words if word.lower() not in my_stopwords]
    words = ["".join(ch for ch in word if ch not in exclude) for word in words]
    clean_text = " ".join(words)    
    return clean_text
true['modified_text'] = true['modified_text'].apply(preprocess_tweet_text)
print(true['modified_text'])
fake['modified_text'] = fake['modified_text'].apply(preprocess_tweet_text)
print(fake['modified_text'])

pip install wordcloud

from wordcloud import WordCloud
from nltk.corpus import subjectivity,stopwords
wc_params = {
    'background_color': 'white',
    'max_words': 2000,
    #'stopwords': STOPWORDS,
    'max_font_size': 156,
    'random_state': 42,
    'width': 800,
    'height': 800,
    'colormap': 'viridis',
}
word_freq = true['modified_text'].str.split(expand=True).stack().value_counts().to_dict()
ag = WordCloud(**wc_params).generate_from_frequencies(word_freq)
plt.imshow(ag, interpolation="bilinear")
plt.show()

word_freq = fake['modified_text'].str.split(expand=True).stack().value_counts().to_dict()
dg = WordCloud(**wc_params).generate_from_frequencies(word_freq)
plt.imshow(dg, interpolation="bilinear")
plt.show()